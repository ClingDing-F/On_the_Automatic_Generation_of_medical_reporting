{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c4f990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T10:08:43.437216Z",
     "start_time": "2023-02-14T10:08:39.938633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 10:08:41.569893: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 10:08:42.565469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kf1d20/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-14 10:08:42.565567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kf1d20/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-02-14 10:08:42.565579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./utils/\")\n",
    "from utils.model import *\n",
    "from utils.dataset import *\n",
    "from utils.loss import *\n",
    "from utils.logger import Logger\n",
    "\n",
    "\n",
    "\n",
    "from GMR import GMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d2c679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T10:08:43.444182Z",
     "start_time": "2023-02-14T10:08:43.439670Z"
    }
   },
   "outputs": [],
   "source": [
    "class JsonReader(object):\n",
    "    def __init__(self, json_file):\n",
    "        self.data = self.__read_json(json_file)\n",
    "        self.keys = list(self.data.keys())\n",
    "\n",
    "    def __read_json(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "        # return self.data[self.keys[item]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8167aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T10:08:43.489777Z",
     "start_time": "2023-02-14T10:08:43.446461Z"
    }
   },
   "outputs": [],
   "source": [
    "class CaptionSampler(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "        self.vocab = self.__init_vocab()\n",
    "        self.tagger = self.__init_tagger()\n",
    "        self.transform = self.__init_transform()\n",
    "        self.data_loader = self.__init_data_loader(self.args.file_list, self.transform)\n",
    "        self.model_state_dict = self.__load_mode_state_dict()\n",
    "\n",
    "        self.extractor = self.__init_visual_extractor()\n",
    "        self.mlc = self.__init_mlc()\n",
    "        self.co_attention = self.__init_co_attention()\n",
    "        self.sentence_model = self.__init_sentence_model()\n",
    "        self.word_model = self.__init_word_word()\n",
    "\n",
    "        self.ce_criterion = self._init_ce_criterion()\n",
    "        self.mse_criterion = self._init_mse_criterion()\n",
    "        self.s=[]\n",
    "        \n",
    "    def __init_vocab(self):\n",
    "        with open(self.args.vocab_path, 'rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "\n",
    "        print(\"Vocab Size:{}\\n\".format(len(vocab)))\n",
    "        print(\"Init_vocab:success\",self.args.vocab_path)\n",
    "        return vocab\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_ce_criterion():\n",
    "        return nn.CrossEntropyLoss(size_average=False, reduce=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_mse_criterion():\n",
    "        return nn.MSELoss()\n",
    "\n",
    "    def __init_tagger(self):\n",
    "        return Tag()\n",
    "    def __init_transform(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((self.args.resize, self.args.resize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "        return transform\n",
    "    \n",
    "    def __init_data_loader(self, file_list, transform):\n",
    "        data_loader = get_loader(image_dir=self.args.path+self.args.image_dir,\n",
    "                                 file_list=file_list,\n",
    "                                 caption_json=self.args.caption_path,\n",
    "                                 vocabulary=self.vocab, transform=transform,\n",
    "                                 batch_size=self.args.batch_size,\n",
    "                                 s_max=self.args.s_max,\n",
    "                                 n_max=self.args.n_max,\n",
    "                                 shuffle=True)\n",
    "        print(\"_init_data_loader \",file_list)\n",
    "        return data_loader\n",
    "    def __load_mode_state_dict(self):\n",
    "        try:\n",
    "            model_state_dict = torch.load(os.path.join(self.args.model_dir, self.args.load_model_path))\n",
    "            print(\"[Load Model-{} Succeed!]\".format(self.args.load_model_path))\n",
    "            print(\"Load From Epoch {}\".format(model_state_dict['epoch']))\n",
    "            return model_state_dict\n",
    "        except Exception as err:\n",
    "            print(\"[Load Model Failed] {}\".format(err))\n",
    "            raise err\n",
    "            \n",
    "    def __init_visual_extractor(self):\n",
    "        model = VisualFeatureExtractor(model_name=self.args.visual_model_name,\n",
    "                                       pretrained=self.args.pretrained)\n",
    "\n",
    "        if self.model_state_dict is not None:\n",
    "            print(\"Visual Extractor Loaded!\")\n",
    "            model.load_state_dict(self.model_state_dict['extractor'])\n",
    "\n",
    "        if self.args.cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __init_mlc(self):\n",
    "        model = MLC(classes=self.args.classes,\n",
    "                    sementic_features_dim=self.args.sementic_features_dim,\n",
    "                    fc_in_features=self.extractor.out_features,\n",
    "                    k=self.args.k)\n",
    "\n",
    "        if self.model_state_dict is not None:\n",
    "            print(\"MLC Loaded!\")\n",
    "            model.load_state_dict(self.model_state_dict['mlc'])\n",
    "\n",
    "        if self.args.cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __init_co_attention(self):\n",
    "        model = CoAttention(version=self.args.attention_version,\n",
    "                            embed_size=self.args.embed_size,\n",
    "                            hidden_size=self.args.hidden_size,\n",
    "                            visual_size=self.extractor.out_features,\n",
    "                            k=self.args.k,\n",
    "                            momentum=self.args.momentum)\n",
    "\n",
    "        if self.model_state_dict is not None:\n",
    "            print(\"Co-Attention Loaded!\")\n",
    "            model.load_state_dict(self.model_state_dict['co_attention'])\n",
    "\n",
    "        if self.args.cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __init_sentence_model(self):\n",
    "        model = SentenceLSTM(version=self.args.sent_version,\n",
    "                             embed_size=self.args.embed_size,\n",
    "                             hidden_size=self.args.hidden_size,\n",
    "                             num_layers=self.args.sentence_num_layers,\n",
    "                             dropout=self.args.dropout,\n",
    "                             momentum=self.args.momentum)\n",
    "\n",
    "        if self.model_state_dict is not None:\n",
    "            print(\"Sentence Model Loaded!\")\n",
    "            model.load_state_dict(self.model_state_dict['sentence_model'])\n",
    "\n",
    "        if self.args.cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __init_word_word(self):\n",
    "        model = WordLSTM(vocab_size=len(self.vocab),\n",
    "                         embed_size=self.args.embed_size,\n",
    "                         hidden_size=self.args.hidden_size,\n",
    "                         num_layers=self.args.word_num_layers,\n",
    "                         n_max=self.args.n_max)\n",
    "\n",
    "        if self.model_state_dict is not None:\n",
    "            print(\"Word Model Loaded!\")\n",
    "            model.load_state_dict(self.model_state_dict['word_model'])\n",
    "\n",
    "        if self.args.cuda:\n",
    "            model = model.cuda()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def __vec2sent(self, array):\n",
    "        sampled_caption = []\n",
    "        for word_id in array:\n",
    "            word = self.vocab.get_word_by_id(word_id)\n",
    "            if word == '<start>':\n",
    "                continue\n",
    "            if word == '<end>' or word == '<pad>':\n",
    "                break\n",
    "            sampled_caption.append(word)\n",
    "        return ' '.join(sampled_caption)\n",
    "    def __to_var(self, x, requires_grad=True):\n",
    "        if self.args.cuda:\n",
    "            x = x.cuda()\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "    \n",
    "    def __save_json(self, result):\n",
    "        result_path = os.path.join(self.args.model_dir, self.args.result_path)\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "        with open(os.path.join(result_path, '{}.json'.format(self.args.result_name)), 'w') as f:\n",
    "            json.dump(result, f)\n",
    "            \n",
    "    def generate(self):\n",
    "        self.extractor.eval()\n",
    "        self.mlc.eval()\n",
    "        self.co_attention.eval()\n",
    "        self.sentence_model.eval()\n",
    "        self.word_model.eval()\n",
    "\n",
    "        progress_bar = tqdm(self.data_loader, desc='Generating')\n",
    "        results = {}\n",
    "\n",
    "        for images, image_id, label, captions, _ in progress_bar:\n",
    "            images = self.__to_var(images, requires_grad=False)\n",
    "            visual_features, avg_features = self.extractor.forward(images)\n",
    "            tags, semantic_features = self.mlc.forward(avg_features)\n",
    "\n",
    "            sentence_states = None\n",
    "            prev_hidden_states = self.__to_var(torch.zeros(images.shape[0], 1, self.args.hidden_size))\n",
    "            pred_sentences = {}\n",
    "            real_sentences = {}\n",
    "            for i in image_id:\n",
    "                pred_sentences[i] = {}\n",
    "                real_sentences[i] = {}\n",
    "\n",
    "            for i in range(self.args.s_max):\n",
    "                ctx, alpha_v, alpha_a = self.co_attention.forward(avg_features, semantic_features, prev_hidden_states)\n",
    "\n",
    "                topic, p_stop, hidden_state, sentence_states = self.sentence_model.forward(ctx,\n",
    "                                                                                          prev_hidden_states,\n",
    "                                                                                          sentence_states)\n",
    "                p_stop = p_stop.squeeze(1)\n",
    "                p_stop = torch.max(p_stop, 1)[1].unsqueeze(1)\n",
    "\n",
    "                start_tokens = np.zeros((topic.shape[0], 1))\n",
    "                start_tokens[:, 0] = self.vocab('<start>')\n",
    "                start_tokens = self.__to_var(torch.Tensor(start_tokens).long(), requires_grad=False)\n",
    "\n",
    "                sampled_ids = self.word_model.sample(topic, start_tokens)\n",
    "                prev_hidden_states = hidden_state\n",
    "\n",
    "                sampled_ids = torch.Tensor(sampled_ids)*p_stop.cpu()\n",
    "\n",
    "                # self._generate_cam(image_id, visual_features, alpha_v, i)\n",
    "\n",
    "                for id, array in zip(image_id, sampled_ids):\n",
    "                    pred_sentences[id][i] = self.__vec2sent(array.cpu().detach().numpy())\n",
    "\n",
    "            for id, array in zip(image_id, captions):\n",
    "                for i, sent in enumerate(array):\n",
    "                    real_sentences[id][i] = self.__vec2sent(sent)\n",
    "\n",
    "            for id, pred_tag, real_tag in zip(image_id, tags, label):\n",
    "                results[id] = {\n",
    "                    'Real Tags': self.tagger.inv_tags2array(real_tag),\n",
    "                    'Pred Tags': self.tagger.array2tags(torch.topk(pred_tag, self.args.k)[1].cpu().detach().numpy()),\n",
    "                    'Pred Sent': pred_sentences[id],\n",
    "                    'Real Sent': real_sentences[id]\n",
    "                }\n",
    "\n",
    "        self.__save_json(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fb557a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-14T10:09:07.706273Z",
     "start_time": "2023-02-14T10:08:43.492291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:1966\n",
      "\n",
      "Init_vocab:success ./data/vocab.pkl\n",
      "_init_data_loader  ./data/test.csv\n",
      "[Load Model-train_best_loss.pth.tar Succeed!]\n",
      "Load From Epoch 200\n",
      "Visual Extractor Loaded!\n",
      "MLC Loaded!\n",
      "Co-Attention Loaded!\n",
      "Sentence Model Loaded!\n",
      "Word Model Loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████████████████████████████████████████████████████████████████████████| 32/32 [00:13<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \"\"\"\n",
    "    Data Argument\n",
    "    \"\"\"\n",
    "    # Path Argument\n",
    "    parser.add_argument('--model_dir', type=str, default='./report_models/v1/20230213-19:50')\n",
    "    parser.add_argument('--path', type=str, default=\"/datasets/kf1d20/Indiana_data/\",\n",
    "                    help='the path for data')\n",
    "    parser.add_argument('--image_dir', type=str, default=\"content/NLMCXR_png/\",\n",
    "                    help='the path for image')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl',\n",
    "                    help='the path for vocabulary object')\n",
    "    parser.add_argument('--caption_path', type=str, default='./data/captions.json',\n",
    "                        help='path for captions')\n",
    "    parser.add_argument('--file_list', type=str, default='./data/test.csv',\n",
    "                    help='the path for test file list')\n",
    "    parser.add_argument('--load_model_path', type=str, default='train_best_loss.pth.tar',\n",
    "                    help='The path of loaded model')\n",
    "\n",
    "    # transforms argument\n",
    "    parser.add_argument('--resize', type=int, default=224,\n",
    "                    help='size for resizing images')\n",
    "\n",
    "    # CAM\n",
    "    parser.add_argument('--cam_size', type=int, default=224)\n",
    "    parser.add_argument('--generate_dir', type=str, default='cam')\n",
    "\n",
    "    # Saved result\n",
    "    parser.add_argument('--result_path', type=str, default='results',\n",
    "                    help='the path for storing results')\n",
    "    parser.add_argument('--result_name', type=str, default='v4',\n",
    "                    help='the name of results')\n",
    "\n",
    "    \"\"\"\n",
    "    Model argument\n",
    "    \"\"\"\n",
    "    parser.add_argument('--momentum', type=int, default=0.1)\n",
    "    # VisualFeatureExtractor\n",
    "    parser.add_argument('--visual_model_name', type=str, default='densenet201',\n",
    "                    help='CNN model name')\n",
    "    parser.add_argument('--pretrained', action='store_true', default=False,\n",
    "                    help='not using pretrained model when training')\n",
    "\n",
    "    # MLC\n",
    "    parser.add_argument('--classes', type=int, default=210)\n",
    "    parser.add_argument('--sementic_features_dim', type=int, default=512)\n",
    "    parser.add_argument('--k', type=int, default=10)\n",
    "\n",
    "    # Co-Attention\n",
    "    parser.add_argument('--attention_version', type=str, default='v1')\n",
    "    parser.add_argument('--embed_size', type=int, default=512)\n",
    "    parser.add_argument('--hidden_size', type=int, default=512)\n",
    "\n",
    "    # Sentence Model\n",
    "    parser.add_argument('--sent_version', type=str, default='v1')\n",
    "    parser.add_argument('--sentence_num_layers', type=int, default=2)\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "\n",
    "    # Word Model\n",
    "    parser.add_argument('--word_num_layers', type=int, default=1)\n",
    "\n",
    "    \"\"\"\n",
    "    Generating Argument\n",
    "    \"\"\"\n",
    "    parser.add_argument('--s_max', type=int, default=6)\n",
    "    parser.add_argument('--n_max', type=int, default=30)\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=16)\n",
    "\n",
    "    # Loss function\n",
    "    parser.add_argument('--lambda_tag', type=float, default=10000)\n",
    "    parser.add_argument('--lambda_stop', type=float, default=10)\n",
    "    parser.add_argument('--lambda_word', type=float, default=1)\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "\n",
    "    torch.cuda.set_device(2)\n",
    "    sampler = CaptionSampler(args)\n",
    "    sampler.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7750c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.018869Z",
     "start_time": "2023-02-12T12:17:07.018852Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d81299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.020293Z",
     "start_time": "2023-02-12T12:17:07.020279Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cab26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.022144Z",
     "start_time": "2023-02-12T12:17:07.022122Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0817684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.023883Z",
     "start_time": "2023-02-12T12:17:07.023861Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f41ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.025372Z",
     "start_time": "2023-02-12T12:17:07.025350Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d23c73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T12:17:07.026906Z",
     "start_time": "2023-02-12T12:17:07.026882Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
